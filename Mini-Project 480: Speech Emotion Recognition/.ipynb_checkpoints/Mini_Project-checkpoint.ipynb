{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5GwTC6xYNnN"
   },
   "source": [
    "# **ISAT 449: Emerging Topics in Applied Data Science**\n",
    "\n",
    "## **Mini-Project: How to Make a Speech Emotion Recognizer Using Python AndScikit-learn**\n",
    "\n",
    "###**Building a Speech Emotion Recognition system that detects emotion from human speech tone using the Scikit-Learn library,Python and Librosa**\n",
    "\n",
    "\n",
    "**What is Speech Emotion Recognition?**\n",
    "\n",
    "Speech Emotion Recognition, abbreviated as SER, is the act of attempting to recognize human emotion and affective states from speech.  This is capitalizing on the fact that voice often reflects underlying emotion through tone and pitch. This is also the phenomenon that animals like dogs and horses employ to be able to understand human emotion.  SER is tough because emotions are subjective and annotating audio is challenging.\n",
    "\n",
    "**What is librosa?**\n",
    "\n",
    "librosa is a Python library for analyzing audio and music. It has a flatter package layout, standardizes interfaces and names, backwards compatibility, modular functions, and readable code. Further, in this Python mini-project, we demonstrate how to install it (and a few other packages) with pip.\n",
    "\n",
    "**What is JupyterLab?**\n",
    "\n",
    "As you have seen, I use JupyterLab which is an open-source, web-based UI for Project Jupyter and it has all basic functionalities of the Jupyter Notebook, like notebooks, terminals, text editors, file browsers, rich outputs, and more. However, it also provides improved support for third party extensions. It comes bubdled with the Anaconda Data Science Framework if you want to try it out, BUT you can just keep using your Jupyter Notebook and all wii be fine.\n",
    "\n",
    "**Speech Emotion Recognition – Objective**\n",
    "\n",
    "To build a model to recognize emotion from speech using the librosa and sklearn libraries and the RAVDESS dataset.\n",
    "\n",
    "**Speech Emotion Recognition – About the Python Mini Project**\n",
    "\n",
    "In this Python mini project, we will use the libraries librosa, soundfile, and sklearn (among others) to build a model using an MLPClassifier. This will be able to recognize emotion from sound files. We will load the data, extract features from it, then split the dataset into training and testing sets. Then, we’ll initialize an MLPClassifier and train the model. Finally, we’ll calculate the accuracy of our model.\n",
    "\n",
    "**The Dataset**\n",
    "\n",
    "For this Python mini project, we’ll use the RAVDESS dataset; this is the Ryerson Audio-Visual Database of Emotional Speech and Songdataset, and is free to download. This dataset has 7356 files rated by 247 individuals 10 times on emotional validity, intensity, and genuineness. The entire dataset is 24.8GB from 24 actors, but we’ve lowered the sample rate on all the files, and you can download from Canvas.\n",
    "\n",
    "**File Summary**\n",
    "\n",
    "In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files).\n",
    "\n",
    "**File naming convention**\n",
    "\n",
    "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
    "\n",
    "**Filename identifiers** \n",
    "\n",
    "\n",
    "\n",
    "*   Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "*   Vocal channel (01 = speech, 02 = song).\n",
    "*   Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = suprised\n",
    "*   Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "*   Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "*   Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "*   Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "*Filename example: 02-01-06-01-02-01-12.mp4*\n",
    "\n",
    "\n",
    "\n",
    "*   Video-only (02)\n",
    "*   Speech (01)\n",
    "*   Fearful\n",
    "*   Normal intensity (01)\n",
    "*   Statement \"dogs\" (02)\n",
    "*   1st Repetition (01)\n",
    "*   12th Actor (12)\n",
    "*   Female, as the actor ID number is even.\n",
    "\n",
    "You can find more information on the file structure and filenames from Zenodo: [Filename References](https://zenodo.org/record/1188976#.X39l7pNKjb3)\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "You'll need to install the following libraries with pip:\n",
    "\n",
    "pip install librosa soundfile numpy sklearn pyaudio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkTP7MSwNcHA"
   },
   "source": [
    "The whole pipeline is as follows (same as any machine learning pipeline):\n",
    "\n",
    "\n",
    "*   Preparing the Dataset: Here, we download and convert the dataset to be suited for extraction.\n",
    "*   Loading the Dataset: This process is about loading the dataset in Python which involves extracting audio features, such as obtaining\n",
    "*   Training the Model: After we prepare and load the dataset, we simply train it on a suited sklearn model.\n",
    "*   Testing the Model: Measuring how good our model is doing.\n",
    "\n",
    "## **NOTE on the dataset and file structure**\n",
    "\n",
    "**BEFORE you begin coding, download, the dataset from canvas and extract it to your project folder. I have modified the foldername in the zipped file so that it is easier to parse (see the file structure I used below).**\n",
    "\n",
    "## **Let's import the dependencies**\n",
    "\n",
    "1.   import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IG_LzMPJNbw7"
   },
   "outputs": [],
   "source": [
    "import soundfile # to read audio file\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import librosa # to extract speech features\n",
    "import glob\n",
    "import os\n",
    "import pickle # to save model after training \n",
    "from sklearn.model_selection import train_test_split # for splitting training and testing\n",
    "from sklearn.neural_network import MLPClassifier # multi-layer perceptron model\n",
    "from sklearn.metrics import accuracy_score # to measure how good we are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RsApoCdPLbb"
   },
   "source": [
    "2.   Define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters-the file name and three Boolean parameters for the three features:\n",
    "3.   **mfcc**: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
    "4.   **chroma**: Pertains to the 12 different pitch classes\n",
    "5.   **mel**: Mel Spectrogram Frequency\n",
    "\n",
    "Open the sound file with soundfile.  SoundFile using with-as so it’s automatically closed once we’re done. Read from it and call it X. Also, get the sample rate. If chroma is True, get the Short-Time Fourier Transform of X.\n",
    "\n",
    "Let result be an empty numpy array. Now, for each feature of the three, if it exists, make a call to the corresponding function from librosa.feature (eg- librosa.feature.mfcc for mfcc), and get the mean value. Call the function hstack() from numpy with result and the feature value, and store this in result. hstack() stacks arrays in sequence horizontally (in a columnar fashion). Then, return the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WLP-DdtmPIK1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma: \n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))     \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rXp0eQSSvZX"
   },
   "source": [
    "3. Now, let’s define a dictionary to hold numbers and the emotions available in the RAVDESS dataset, and a list to hold those we want-calm, happy, fearful, disgust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J7u_36pmSs7m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Emotions in the RAVDESS dataset\n",
    "emotions = {\n",
    "    '01':'neutral',\n",
    "    '02':'calm',\n",
    "    '03':'happy',\n",
    "    '04':'sad',\n",
    "    '05':'angry',\n",
    "    '06':'fearful',\n",
    "    '07':'disgust',\n",
    "    '08':'surprised'\n",
    "}\n",
    "\n",
    "# Emotions to observe\n",
    "observed_emotions = ['calm', 'happy', 'fearful', 'disgust']\n",
    "\n",
    "print(glob.glob(\"ravdess-data\\\\Actor_*\\\\*.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EFdO49oTVz1"
   },
   "source": [
    "4.   Now, let’s load the data with a function load_data() – this takes in the   relative size of the test set as parameter. x and y are empty lists; we’ll use the glob() function from the glob module to get all the path names for the sound files in our dataset. The pattern we use for this is: “ravdess-data\\Actor_\\.wav”. This is because our dataset looks like this:\n",
    "\n",
    "**So, for each such path, get the basename of the file, the emotion by splitting the name around ‘-’ and extracting the third value:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IWpI2Q8KTT7Y"
   },
   "outputs": [],
   "source": [
    "# Load the data and extract features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"ravdess-data/Actor_*/*.wav\"):\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size = test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYdkJj1QX3AY"
   },
   "source": [
    "Using our emotions dictionary, this number is turned into an emotion, and our function checks whether this emotion is in our list of observed_emotions; if not, it continues to the next file. It makes a call to extract_feature and stores what is returned in ‘feature’. Then, it appends the feature to x and the emotion to y. So, the list x holds the features and y holds the emotions.\n",
    "\n",
    "We call the function train_test_split with these, the test size, and a random state value, and return that.\n",
    "\n",
    "5. Time to split the dataset into training and testing sets! Let’s keep the test set 25% of everything and use the load_data function for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "-CtbeRBoXxsZ",
    "outputId": "a07cfe31-252a-4b8b-d9a8-d1cae5f000f2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "x_train, x_test, y_train, y_test = load_data(test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmOF1JZ9YVFM"
   },
   "source": [
    "6. Observe the shape of the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DDRxteKRYXC0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 192)\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the training and testing datasets\n",
    "print((x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDwhRYkmYqdo"
   },
   "source": [
    "7. And get the number of features extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MVLoXFawYqHS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "# Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlYCqla6ZUzW"
   },
   "source": [
    "8. Now, let’s initialize an MLPClassifier. This is a Multi-layer Perceptron Classifier; it optimizes the log-loss function using differentoptimizers (lbfgs, adam or stochastic gradient descent). Recall that the MLPClassifier has an internal neural network for the purposeof classification, and is a feed-forward ANN model.\n",
    "\n",
    "I pretty much played around with the parmaeters of the MLPClassifier to get the best possible hyper parameters. These gave me anaccuracy of 75-80%. You can modify them to see if you can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Tnnd2DD8Z5tz"
   },
   "outputs": [],
   "source": [
    "# Initialize the Multi Layer Perceptron Classifier\n",
    "model = MLPClassifier(solver='adam', alpha=0.005, batch_size=200, epsilon=1e-08,\n",
    "                      hidden_layer_sizes=(400,),learning_rate='adaptive', max_iter=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoDsWnpOZJD1"
   },
   "source": [
    "9. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u2g6WsVdaElz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.005, batch_size=200, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(400,), learning_rate='adaptive',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=700,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfnaCbUDaNDT"
   },
   "source": [
    "10. Plot the loss during training\n",
    "\n",
    "A loss function, also known as a cost function, takes into account the probabilities or uncertainty of a prediction based on how much the prediction varies from the true value. This gives us a more nuanced view into how well the model is performing.\n",
    "\n",
    "Unlike accuracy, loss is not a percentage — it is a summation of the errors made for each sample in training or validation sets. Loss is often used in the training process to find the \"best\" parameter values for the model (e.g. weights in neural network). During the training process the goal is to minimize this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i6sxq4x0aMSs"
   },
   "outputs": [],
   "source": [
    "loss_values = model.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(6, 4))\n",
    "plt.plot(loss_values, color=\"red\", label='training loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss');\n",
    "#legend\n",
    "plt.legend(loc='upper right')\n",
    "#title\n",
    "plt.title('Training Loss Per Epoch: Speech Emotion Recognition')\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. <b> Model Predictions </b>\n",
    "\n",
    "To calculate the accuracy of our model, we’ll call up the accuracy_score() function we imported from sklearn. Finally, we’ll round the accuracy to 2 decimal places and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for the test set\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. <b> Model Performance </b>\n",
    "\n",
    "To calculate the accuracy of our model, we’ll call up the accuracy_score() function we imported from sklearn. Finally, we’ll round the accuracy to 2 decimal places and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.52%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of our model\n",
    "accuracy = accuracy_score(y_true = y_test, y_pred=y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accurracy: 75.52 %\n"
     ]
    }
   ],
   "source": [
    "print('test set accurracy: {0:.2f} %'.format(100*model.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Peformance on a single waveform</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predictied emotion from the speech is: fearful\n"
     ]
    }
   ],
   "source": [
    "print('The predictied emotion from the speech is: {}'      \n",
    "      .format(model.predict([x_test[22]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true emotion is: happy\n"
     ]
    }
   ],
   "source": [
    "print('The true emotion is: {}'.format(y_test[22]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Pickle (serialize) and save the trained classifier to a folder </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization with Python's Pickle\n",
    "import pickle \n",
    "import os\n",
    "\n",
    "dest=os.path.join('classifiers', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "    \n",
    "pickle.dump(model, open(os.path.join(dest, 'ser_classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load the saved classifier into memory </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved the trained classifier into memory\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.chdir('classifiers')\n",
    "ser_model=pickle.load(open(os.path.join('pkl_objects', 'ser_classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Test the re-loaded model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The reloaded model predicts the  emotion from the speech is:{}'      \n",
    "      .format(ser_model.predict([x_test[33]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The true emotion is: {}'.format(y_test[33]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Summary </b> \n",
    "\n",
    "In this Python, Machine Learning, Mini-Project, we learned to recognize emotions from speech. We used an MLPClassifier for the neural netwoerk and and made use of the soundfile library to read the sound file, and the librosa library to extract features from it.  We also serialized the model and tested the reloaded model on unseen data and saw that it performed well. So, as you've seen, the model delivered an accuracy of 70-80%, which is not bad for our first attempt at building a speech emotion recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Mini-Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
